# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pp0roUaumR39y6SD8bCyO37ts6jDVwuo

#NLP Corona Tweet Analysis

In this project I used machine learning techniques with text, and using math and statistics to get that text in a format that the machine learning algorithms can understand!

The dataset Corona_NLP_train.csv is available on kaggel, which contains 6 instances:

Username
Screenname
Location
TweetAt
OriginalTweet
Sentiment

This dataset contains 40k+ records.

https://www.kaggle.com/datatattle/covid-19-nlp-text-classification?select=Corona_NLP_train.csv

Download stopwords package from nltk library
"""

import nltk 
nltk.download_shell()



"""Import required libraries"""

import numpy as np
import pandas as pd

"""Load Dataset"""

#here we used encoding=latin1 because some of the data in dataset are utf-8 encoding format
df = pd.read_csv('Corona_NLP_train.csv',encoding='latin1')

"""#Data Cleaning & Preprocessing"""

df.head()

df.info()

df.describe()

df['Sentiment'].value_counts()

"""Adding 2 values "Extremely Positive" and "Extremely Negative" in Positive & Negative."""

df['Sentiment']=df['Sentiment'].replace(['Extremely Positive'], 'Positive')

df['Sentiment']=df['Sentiment'].replace(['Extremely Negative'], 'Negative')

"""Converting Values of Sentiment from Categorical to Numerical"""

'''from sklearn.preprocessing import LabelEncoder 

df['sentiment'] = LabelEncoder().fit_transform(df['Sentiment']) 

df[['Sentiment', 'sentiment']] # special syntax to get just these two columns 

df = df.drop('Sentiment',axis=1) '''

"""Removing Values for Sentiment=="Neutral""""

df = df[df['Sentiment']!='Neutral']
# ---------------- OR ---------------------
#df.drop(df.loc[df['Sentiment']=='Neutral'].index, inplace=True)

"""Handling or Removing NaN values """

df.isnull().sum()

df = df.dropna()

df.head()

df['Sentiment'].value_counts()

"""Adding new column text length in dataset which contains len of original tweet"""

df['text length'] = df['OriginalTweet'].apply(len)

df.describe()

"""# EDA Exploratory Data Analysis

Let's explore the data

## Imports

**Import the data visualization libraries.**
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('white')
# %matplotlib inline
import plotly.express as px

g = sns.FacetGrid(df,col='Sentiment')
g.map(plt.hist,'text length')

plt.figure(figsize=(12,12))
px.histogram(x=df['text length'],color=df['Sentiment'],nbins=40)

sns.boxplot(x='Sentiment',y='text length',data=df,palette='rainbow')

px.box(x=df['Sentiment'],y=df['text length'])

stars = df.groupby('Sentiment').mean()
stars.iloc[:,2:]

df.head()

"""Drop Unnecessary Instances"""

df = df.drop(['UserName','ScreenName','Location','TweetAt'],axis=1)

df.groupby('Sentiment').describe()

df['text length'].describe()

df[df['text length']==355]['OriginalTweet'].iloc[0]

"""Remove punctuation "!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~" from string"""

import string

mess = 'Hello! I am Sagar : '

string.punctuation

import string


# Check characters to see if they are in punctuation
nopunc = [char for char in mess if char not in string.punctuation]

# Join the characters again to form the string.
nopunc = ''.join(nopunc)

nopunc

"""Import stopwords for english dictionary which stores common english words."""

from nltk.corpus import stopwords

stopwords.words('english')[0:5]

"""Remove Unnecessary words which are very common "i","my" etc."""

# Now just remove any stopwords
clean_mess = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]

clean_mess

"""Create function text_process to apply all this filtering on dataset """

def text_process(mess):
    """
    Takes in a string of text, then performs the following:
    1. Remove all punctuation
    2. Remove all stopwords
    3. Returns a list of the cleaned text
    """
    # Check characters to see if they are in punctuation
    nopunc = [char for char in mess if char not in string.punctuation]

    # Join the characters again to form the string.
    nopunc = ''.join(nopunc)
    
    # Now just remove any stopwords
    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]

# Check to make sure its working
df['OriginalTweet'].head(5).apply(text_process)

"""import 'countvectorizer' to calculate howmany times a particular word occurs in column 'OriginalTweet'"""

from sklearn.feature_extraction.text import CountVectorizer

# Might take awhile...
bow_transformer = CountVectorizer(analyzer=text_process).fit(df['OriginalTweet'])

# Print total number of vocab words in column 'OriginalTweet'
print(len(bow_transformer.vocabulary_))

tweet5 = df['OriginalTweet'].iloc[3]
print(tweet5)

bow4 = bow_transformer.transform([tweet5])
print(bow4)
print(bow4.shape)

print(bow_transformer.get_feature_names()[13185])

tweet_bow = bow_transformer.transform(df['OriginalTweet'])

"""Creating Sparse Matrix

Advantage of sparse matrix : Save storage & time to execute

Sparse Matrix: https://medium.com/@shachiakyaagba_41915/the-sparse-matrix-with-nlp-77901216c649
"""

print('Shape of Sparse Matrix: ', tweet_bow.shape)
print('Amount of Non-Zero occurences: ', tweet_bow.nnz)

"""Calculating Sparsity"""

#Sparsity and density are terms used to describe the percentage of cells in a database table that are not populated and populated, respectively. The sum of the sparsity and density should equal 100%. 
# sparsity = 100 * number of non-zero occurences / (number of rows * number of columns)
sparsity = (100.0 * tweet_bow.nnz / (tweet_bow.shape[0] * tweet_bow.shape[1]))
print('sparsity: {}'.format(sparsity))
# sparsity = 0.026 which means only 2% of datas are non zero from matrix

"""Import TfidfTransfromer to transform sentences in words and calculate frequency of all words so according to that we can priortise which words important."""

from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer().fit(tweet_bow)
tfidf4 = tfidf_transformer.transform(bow4)
print(tfidf4)

print(tfidf_transformer.idf_[bow_transformer.vocabulary_['talking']])
print(tfidf_transformer.idf_[bow_transformer.vocabulary_['university']])

tweet_tfidf = tfidf_transformer.transform(tweet_bow)
print(tweet_tfidf.shape)

"""# Applying Naive Bayes algo. on data
https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf
"""

from sklearn.naive_bayes import MultinomialNB
corona_detect_model = MultinomialNB().fit(tweet_tfidf, df['Sentiment'])



print('predicted:', corona_detect_model.predict(tfidf4))
print('expected:', df.Sentiment.iloc[4])

all_predictions = corona_detect_model.predict(tweet_tfidf)
print(all_predictions)

from sklearn.metrics import classification_report
print (classification_report(df['Sentiment'], all_predictions))

"""#Creating Pipeline Model

https://medium.com/analytics-vidhya/clean-data-science-workflow-with-sklearn-pipeline-2b648634b29f
"""

from sklearn.model_selection import train_test_split

tweet_train, tweet_test, sentiment_train, sentiment_test = \
train_test_split(df['OriginalTweet'], df['Sentiment'], test_size=0.2)

print(len(tweet_train), len(tweet_test), len(tweet_train) + len(tweet_test))



from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts
    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores
    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier
])

pipeline.fit(tweet_train,sentiment_train)

predictions = pipeline.predict(tweet_test)

print(classification_report(predictions,sentiment_test))



"""#Testing both the Models

Taking random tweet related to corona and testing with our model.
"""

rand = "Even if you are infected with the corona virus and you report this via the app, this cannot be linked to your name or contact details."

bow4 = bow_transformer.transform([rand])
print(bow4)
print(bow4.shape)
print("------------------------------------------------------------------------")
tfidf4 = tfidf_transformer.transform(bow4)
print(tfidf4)

print("------------------------------------------------------------------------")
print('predicted:', corona_detect_model.predict(tfidf4)[0])
print("------------------------------------------------------------------------")


ml = pd.DataFrame(tweet_test.head(1))

ml['OriginalTweet']=ml['OriginalTweet'].replace(ml['OriginalTweet'].iloc[0], rand)
print("Predicted by Pipeline Model: ",pipeline.predict(ml)[0])

tweet5 = "Okay so I just think sense Nancy Pelosi tested positive for the corona virus Microbe, does that mean she’s not going to be able to get out of the house for 9 months and not go to restaurants and follow her Governor Newsom’s orders,for California citizens."
print(tweet5)

bow4 = bow_transformer.transform([tweet5])
print(bow4)
print(bow4.shape)

tfidf4 = tfidf_transformer.transform(bow4)
print(tfidf4)

print('predicted:', corona_detect_model.predict(tfidf4)[0])
print("------------------------------------------------------------------------")

ml = pd.DataFrame(tweet_test.head(1))

ml=ml['OriginalTweet'].replace(ml['OriginalTweet'].iloc[0],tweet5)

print("Predicted by Pipeline Model: ",pipeline.predict(ml)[0])